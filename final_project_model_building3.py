# -*- coding: utf-8 -*-
"""Final Project Model Building3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CsE3Nt5ncaFInaep_c8WCnICBVolAZhJ
"""

# get colab correct settings
# !cat /proc/cpuinfo
# !cat /proc/meminfo

# gpu_info = !nvidia-smi
# gpu_info = '\n'.join(gpu_info)
# if gpu_info.find('failed') >= 0:
#   print('Not connected to a GPU')
# else:
#   print(gpu_info)

# from psutil import virtual_memory
# ram_gb = virtual_memory().total / 1e9
# print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

# if ram_gb < 20:
#   print('Not using a high-RAM runtime')
# else:
#   print('You are using a high-RAM runtime!')

"""# 1111 - HDF+MDF+LSTM+CA"""

# from gensim.models import doc2vec
import pandas as pd
# from collections import namedtuple
# import scipy.io as sio

HF_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CS598 DLH/Final Data/HF_Final_New.csv')

print(HF_data[0:3])
# print(HF_data.columns[2:11])
# # HF_data.iloc[1:4,11:211]
# HF_data.iloc[1:4,211]

HF_data.loc[HF_data["Weight"] == 2, "Weight"] = 3

print(HF_data[460:465]["Weight"])

"""- columns 2-10  HDF
- columns 11-210 MDF
- columns 211-212:  Weight, Flag30

# Load Data
"""

import torch
import numpy as np
from torchvision import datasets
from torchvision import transforms
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import Dataset

from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F

DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CS598 DLH/Final Data/HF_Final_New.feather'
LABEL_COL = 212
WEIGHT_COL = 211
VID_COL = 1
PID_COL = 0
MAX_VISIT_LEN = 42

class CustomDataset(Dataset):
    def __init__(self, file_path, HMLC):  # 42 visits length
        # Read feature file in the drive
        # HMLC = '1111'
        # MAX_VISIT_LEN = 42
        self.HF_data = pd.read_feather(file_path, columns=None, use_threads=True)
        self.HF_data.loc[self.HF_data["Weight"] == 2, "Weight"] = 3
        self.y_idx = LABEL_COL
        self.weight_idx = WEIGHT_COL
        self.total_length = len(self.HF_data)
        self.num_patients = int(self.total_length/MAX_VISIT_LEN)
        # print(self.num_patients)  #9235

        # your code here
        if HMLC[0:2] == '11':
          self.ind1 = 2
          self.ind2 = 211
        elif HMLC[0:2] == '10':
          self.ind1 = 2
          self.ind2 = 11
        else:
          self.ind1 = 11
          self.ind2 = 211
        self.patients = []
        self.x = []
        self.y = []
        self.weights = []
         #0-9234
        for n in range(self.num_patients):
          self.x.append(self.HF_data.iloc[n*MAX_VISIT_LEN:(n+1)*MAX_VISIT_LEN,self.ind1:self.ind2].values) 
          self.y.append(self.HF_data.iloc[n*MAX_VISIT_LEN:(n+1)*MAX_VISIT_LEN,self.y_idx].values)
          self.weights.append(self.HF_data.iloc[n*MAX_VISIT_LEN:(n+1)*MAX_VISIT_LEN,self.weight_idx].values)


    def __len__(self): #TODO: Return the number of samples (i.e. patients).
             
        return len(self.y)
    
    def __getitem__(self, index): #Generates one sample of data. no need to convert to tensor for now

        return self.x[index],self.y[index],self.weights[index]

dataset = CustomDataset(DATA_PATH, '1111')

# print(len(dataset))
# print(len(dataset[0]))
# print(len(dataset[0][1]))
# print(dataset[0][2])
# print(dataset[0][0].shape)
# print(len(dataset))

def collate_fn(data):

  # num_visits = MAX_VISIT_LEN
  num_features = len(data[0][0][0])   # feature
  num_patients = len(data)        # batch
  num_visits = len(data[0][1])
  # print(num_features)
  # print(num_patients)
  # print(num_visits)
  # print(data)
  # print(data[0][1])
  
  x = torch.zeros((num_patients,num_visits,num_features),dtype = torch.float)
  weights = torch.zeros((num_patients,num_visits),dtype = torch.float)
  y = torch.zeros((num_patients,num_visits),dtype= torch.float)
  
  for i_patient, patient in enumerate(data):
    x[i_patient] = torch.from_numpy(patient[0])
    y[i_patient] = torch.from_numpy(patient[1])
    weights[i_patient] = torch.from_numpy(patient[2])
  y = torch.unsqueeze(y, 2)
  weights = torch.unsqueeze(weights, 2)
  return x,y,weights

# loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)
# loader_iter = iter(loader)
# a,b,c = next(loader_iter)
# print(a.shape)
# print(b.shape)
# print(c)

from torch.utils.data.dataset import random_split

split = int(len(dataset)*0.7)
lengths = [split, len(dataset) - split]

train_dataset, val_dataset = random_split(dataset, lengths)

print("Length of train dataset:", len(train_dataset))
print("Length of val dataset:", len(val_dataset))

"""# DataLoader"""

from torch.utils.data import DataLoader

def load_data(train_dataset, val_dataset, collate_fn):
    batch_size = 32
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                        shuffle=True,collate_fn = collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                        shuffle=True,collate_fn = collate_fn)
    
    return train_loader, val_loader

train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)

for x,y,weights in train_loader:   #testing    
    print(x.shape)
    print(y.shape)
    print (weights.shape)
    break

# x shape(batch,M(visit_length),features)
# y shape(batch,M(visit_length))
# weights shape(batch,M(visit_length))

print(len(train_loader))
print(len(val_loader))



class LSTM_Network(nn.Module):
  def __init__(self,number_features=209):
    super().__init__()
    """  No need for embedding as we have already done that
    TODO: 
        1. Define FC layer with 128 nodes
        2. Define Sigmoid activation 
        3. Define LSTM layer with  hidden size:64  input size:128  batch: true
        4. Define FC layer with 32 nodes
        5. Define Sigmoid activation            
        6. Define FC layer with 1 nodes
        4. Define the final activation layer using `nn.Sigmoid().
    """ 
    self.fc1 = nn.Linear(number_features,128)
    self.sigmoid1 = nn.Sigmoid()
    self.lstm = nn.LSTM(128, 64, num_layers=1, batch_first = True)
    self.fc2 = nn.Linear(64,32)
    self.sigmoid2 = nn.Sigmoid()
    self.fc3 = nn.Linear(32,1)
    self.sigmoid3 = nn.Sigmoid()


  def forward(self, x):
    batch_size = x.shape[0]
    x = self.fc1(x)
    x = self.sigmoid1(x)
    x,_ = self.lstm(x)
    x = self.fc2(x)
    x = self.sigmoid2(x)
    x = self.fc3(x)
    out = self.sigmoid3(x)

    return out

# load the model here
lstm_rnn = LSTM_Network()
lstm_rnn

"""# Model Training

## Loss and Optimizer
"""

def weighted_binary_cross_entropy(output, target, weights=None):
        
    if weights is not None:
        
        
        loss = weights * (target * torch.log(output)) + \
               weights * ((1 - target) * torch.log(1 - output))
    else:
        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)

    return torch.neg(torch.mean(loss))

# criterion = nn.BCELoss()
optimizer = torch.optim.Adam(lstm_rnn.parameters(),lr=0.001)

"""## Evaluate"""

from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
def eval_model(model, val_loader):    
   
    model.eval()
    y_pred = torch.LongTensor()
    y_score = torch.Tensor()
    y_true = torch.LongTensor()
    model.eval()
    for x, y, weights in val_loader:
        y_hat = model(x)
        #print(y_hat)
        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)
        y_hat = (y_hat > 0.4).int()
        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)
        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)
    # print(y_hat)
    y_true = torch.flatten(y_true)
    y_pred = torch.flatten(y_pred)
    y_score = torch.flatten(y_score)

    p, r, f, roc_auc = None, None, None, None    
    p,r,f, _ = precision_recall_fscore_support(y_true,y_pred,average='binary')
    roc_auc = roc_auc_score(y_true,y_score)

    return p, r, f, roc_auc

"""## Training"""

def train(model, train_loader, val_loader, n_epochs):
    
    for epoch in range(n_epochs):
        model.train()
        train_loss = 0
        for x,y,weights in train_loader:
            """
            TODO:
                1. zero grad
                2. model forward
                3. calculate loss
                4. loss backward
                5. optimizer step
            """            
            loss = None
            # your code here
            optimizer.zero_grad()
            outputs = model(x)
            loss = weighted_binary_cross_entropy(outputs, y,weights)
            loss.backward()
            optimizer.step()
            #raise NotImplementedError
            train_loss += loss.item()
        train_loss = train_loss / len(train_loader)
        print('Epoch: {} \t Training Loss: {:.6f}'.format(epoch+1, train_loss))
        p, r, f, roc_auc = eval_model(model, val_loader)
        print('Epoch: {} \t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'
              .format(epoch+1, p, r, f, roc_auc))

n_epochs = 500
train(lstm_rnn, train_loader, val_loader, n_epochs)