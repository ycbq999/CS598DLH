# -*- coding: utf-8 -*-
"""CS598_DLH_FinalProject_dataprocess_D2V_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d1FrgWp1UQ_m5tFVeNhr-OH6ksuRReML

https://www.youtube.com/watch?v=IRKDrrzh4dE&ab_channel=Quantopian

[project reference](https://github.com/caisr-hh/lstm_predict)
"""

# get colab correct settings
# !cat /proc/cpuinfo
# !cat /proc/meminfo
# gpu_info = !nvidia-smi
# gpu_info = '\n'.join(gpu_info)
# if gpu_info.find('failed') >= 0:
#   print('Not connected to a GPU')
# else:
#   print(gpu_info)

# from psutil import virtual_memory
# ram_gb = virtual_memory().total / 1e9
# print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

# if ram_gb < 20:
#   print('Not using a high-RAM runtime')
# else:
#   print('You are using a high-RAM runtime!')

from gensim.models import doc2vec
import pandas as pd
from collections import namedtuple
import scipy.io as sio

patients_data = pd.read_csv('/content/HF_ADMISSIONS.csv')
print(patients_data)

HF_Admission_Path = '/content/HF_ADMISSIONS.csv'
patient_visit = {}

infd = open(HF_Admission_Path, 'r')
infd.readline()
for line in infd:
    tokens = line.strip().split(',')
    PID = (tokens[0])
    VID = (tokens[1])

    if PID in patient_visit:
        patient_visit[PID].append(VID)
    else:
        patient_visit[PID] = [VID]
infd.close()

patients = list(patient_visit.keys())
print(patients)
visit_list_per_patient = list(patient_visit.values())
print(visit_list_per_patient)
print(len(patients))
print(len(visit_list_per_patient))

def FindMaxLength(lst):
    maxList = max(lst, key = len)
    maxLength = max(map(len, lst))
      
    return maxList, maxLength

maxVistList, maxVisitLength = FindMaxLength(visit_list_per_patient)
print(maxVistList)
print(maxVisitLength)

padding_length = maxVisitLength

# mask dummy visit
count = 0
padded_visit_list_per_patient = []
print(visit_list_per_patient)
for visit in visit_list_per_patient:
  dummy_vid_per_patient_list = []
  for i in range(padding_length - len(visit)):
    # print(i)
    dumm_vid = 'NA'+str(count)
    dummy_vid_per_patient_list.append(dumm_vid)
    count = count+1
  # print(dummy_vid_per_patient_list)
  dummy_vid_per_patient_list.extend(visit)  
  # print(dummy_vid_per_patient_list)
  padded_visit_list_per_patient.append(dummy_vid_per_patient_list)

print(padded_visit_list_per_patient[0])
print(len(padded_visit_list_per_patient[0]))
print(len(padded_visit_list_per_patient))

padded_patients = []
for i in patients:
  p =[i]*padding_length
  padded_patients.append(p)

print(padded_patients[0])
print(len(padded_patients[0]))
print(len(padded_patients))

flat_padded_patients = []
for sublist in padded_patients:
    for item in sublist:
        flat_padded_patients.append(item)

print(len(flat_padded_patients))

flat_padded_vid_per_patient = []
for sublist in padded_visit_list_per_patient:
    for item in sublist:
        flat_padded_vid_per_patient.append(item)

print(len(flat_padded_vid_per_patient))
print(flat_padded_vid_per_patient[41])

#flat_padded_vid_per_patient   flat_padded_patients
df = pd.DataFrame(list(zip(flat_padded_patients, flat_padded_vid_per_patient)),
               columns =['SUBJECT_ID', 'HADM_ID'])

print(df)

HF_ADMISSIONS = pd.read_csv('/content/HF_ADMISSIONS.csv')
HF_ADMISSIONS['HADM_ID']=HF_ADMISSIONS['HADM_ID'].astype("string")
df['HADM_ID']=df['HADM_ID'].astype("string")
NEW_DF = df.merge(HF_ADMISSIONS, on='HADM_ID',how = 'left')

print(NEW_DF)

NEW_DF = NEW_DF.drop(['SUBJECT_ID_y'], axis=1)

NEW_DF['AGE'] = NEW_DF['AGE'].fillna(-1)

NEW_DF['GENDER'] = NEW_DF['GENDER'].fillna(-1)
NEW_DF['PROCEDURE_COUNT'] = NEW_DF['PROCEDURE_COUNT'].fillna(-1)
NEW_DF['DURATION_STAY'] = NEW_DF['DURATION_STAY'].fillna(-1)
NEW_DF['DOAS'] = NEW_DF['DOAS'].fillna(-1)
NEW_DF['VISIT_TYPE'] = NEW_DF['VISIT_TYPE'].fillna(-1)
NEW_DF['CCI'] = NEW_DF['CCI'].fillna(-1)
NEW_DF['NUM_PREVOUS_ICUS'] = NEW_DF['NUM_PREVOUS_ICUS'].fillna(-1)
NEW_DF['NUM_PRE_ADM'] = NEW_DF['NUM_PRE_ADM'].fillna(-1)
NEW_DF['FLAG30'] = NEW_DF['FLAG30'].fillna(0)

print(NEW_DF)
NEW_DF.to_feather('NEW_DF.feather')

# print(flat_padded_vid_per_patient[1:100])

dummy_vid = list(NEW_DF['HADM_ID'].loc[NEW_DF['HADM_ID'].str.startswith('NA', na=False)])
print(len(dummy_vid))
dummy_icd9 = ['N0']*len(dummy_vid)
dummy_adm_icd9 = pd.DataFrame(list(zip(dummy_vid, dummy_icd9)),
               columns =['HADM_ID', 'ICD9_CODE'])

print(dummy_adm_icd9)

ADM_ICD9 = pd.read_csv('/content/ADMISSION_ICD9.csv')
print(ADM_ICD9)

NEW_admission_icd9 = pd.concat([ADM_ICD9,dummy_adm_icd9],ignore_index = True)

print(NEW_admission_icd9)

NEW_admission_icd9.to_csv('NEW_admission_icd9.csv', index=False)

"""# Data new """

#Testing From Author's Code
D2V_data_path = '/content/NEW_admission_icd9.csv'
admDiagMap = {}
infd = open(D2V_data_path, 'r')
infd.readline()
for line in infd:
    tokens = line.strip().split(',')
    admId = (tokens[0])
    diagId = (tokens[1])
    if admId in admDiagMap:
        admDiagMap[admId].append(diagId)
    else:
        admDiagMap[admId] = [diagId]
infd.close()

# From Author's Code
s1 = list(admDiagMap.values())
print(s1[0:3])
print(s1[len(s1)-1])
print(len(s1))
s0 = list(admDiagMap.keys())
print(s0[0:3])
print(s0[len(s0)-1])
print(len(s0))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(s1):
    words = text
    tags = [i]
    docs.append(analyzedDocument(words, tags))
print(docs[387869])

longest_len = max(map(len, admDiagMap.values()))
max_lens = [k for k, v in admDiagMap.items() if len(v) == longest_len]
print(f'longest length(most diagnosis):{longest_len}')
print(f'print the admID which has most visits: {max_lens}')

print(len(docs))
Emb_size=200 #15342         https://datascience.stackexchange.com/questions/51404/word2vec-how-to-choose-the-embedding-size-parameter
print(max(map(len, s1))) #max code in the list 1818
window = max(map(len, s1))  #1818   # Max length of codes in any visit
min_count=0 # Consider all codes
ns=20 # Negative sampling  ?????????????
ns_exponent=-0.75 # Negative because we like to account for rare clinical events??????????????
dm=0 # for PV-DBOW?????????????

model = doc2vec.Doc2Vec(docs, vector_size = Emb_size, window = window, min_count = min_count, workers = 4,negative =ns, ns_exponent=ns_exponent, dm=dm)

# Get the vectorsand save
d2v=model.docvecs.vectors_docs 
sio.savemat('d2v_200.mat', {'d2v_200':d2v})

"""https://towardsdatascience.com/how-to-load-matlab-mat-files-in-python-1f200e1287b5"""

test = sio.loadmat('/content/d2v_200.mat')

# print(test)
# for k in test.keys():
#     if not k.startswith('__'):
#         print(f'k:{k}')
#         print(k + " " + test[k].dtype.name + " " + str(test[k].shape))
# print(test['d2v_200'])

# print(test['d2v_200'])

col_count = len(test['d2v_200'][0])
row_count = len(test['d2v_200'])
col_names = []
for i in range(col_count):
  name = 'd2vRNN'+ str(i)
  col_names.append(name)
print(col_names)

df = pd.DataFrame(test['d2v_200'],columns=col_names)

df.insert(0, "HADM_ID", s0, True)
print(df)

HF_ADMISSIONS = pd.read_feather('/content/NEW_DF.feather', columns=None, use_threads=True)
print(HF_ADMISSIONS)

HF_ADMISSIONS['HADM_ID']=HF_ADMISSIONS['HADM_ID'].astype('string')
df['HADM_ID']=df['HADM_ID'].astype('string')

HF_Final_New = HF_ADMISSIONS.merge(df, on='HADM_ID')
print(HF_Final_New)

col_count = len(HF_Final_New.columns)
y = HF_Final_New.pop('FLAG30')
HF_Final_New.insert(col_count-1, 'FLAG30', y)
print(HF_Final_New)

HF_Final_New.rename(columns = {'SUBJECT_ID_x':'SUBJECT_ID'}, inplace = True)
print(HF_Final_New)

HF_Final_New.to_csv('HF_Final_New.csv', index=False)

HF_Final_New.to_feather('HF_Final_New.feather')

dataset_test = pd.read_feather('/content/HF_Final_New.feather', columns=None, use_threads=True)

print(dataset_test)